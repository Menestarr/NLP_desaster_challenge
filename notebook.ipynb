{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "No target: What's up man?\n",
      "Target: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(f\"No target: {train[train[\"target\"] == 0][\"text\"].values[0]}\")\n",
    "print(f\"Target: {train[train[\"target\"] == 1][\"text\"].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31                         Birmingham\n",
       "32      Est. September 2012 - Bristol\n",
       "33                             AFRICA\n",
       "34                   Philadelphia, PA\n",
       "35                         London, UK\n",
       "                    ...              \n",
       "7575                               TN\n",
       "7577           #NewcastleuponTyne #UK\n",
       "7579                Vancouver, Canada\n",
       "7580                          London \n",
       "7581                          Lincoln\n",
       "Name: location, Length: 5080, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['location'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "def text_preprocessing(df):\n",
    "    df['text_cleaned'] = df['text'].copy()\n",
    "    # Remove links\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.replace(r'http\\S+|www\\.\\S+', '', regex=True)\n",
    "    # Remove mentions\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.replace(r'@\\w+', '', regex=True)\n",
    "    # Remove special caracters\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.replace(r'[^a-zA-Z0-9 ]', '', regex=True)\n",
    "    # Remove trailing sapces\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.strip()\n",
    "    # Remove multiple spaces\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    # Lower\n",
    "    df['text_cleaned'] = df['text_cleaned'].str.lower()\n",
    "    return df\n",
    "    \n",
    "def extract_cities_countries_continents(location):\n",
    "    continents = [\"africa\", \"europe\", \"asia\", \"north america\", \"south america\", \"antartica\", \"oceania\", \"Nigeria\", \"Kenya\"]\n",
    "    countries = [\n",
    "        ('france', 'europe'),\n",
    "        ('united states', 'north america'),\n",
    "        ('united kingdom', 'europe'),\n",
    "        ('germany', 'europe'),\n",
    "        ('india', 'asia'),\n",
    "        ('australia', 'oceania'),\n",
    "        ('canada', 'north america'),\n",
    "        ('japan', 'asia'),\n",
    "        ('indonesia', 'asia'),\n",
    "        ('ireland', 'europe'),\n",
    "    ]\n",
    "    cities = [\n",
    "        ('new york', 'united states', \"north america\"),\n",
    "        ('london', 'united kingdom', \"europe\"),\n",
    "        ('los angeles', 'united states', \"north america\"),\n",
    "        ('mumbai', 'india', \"asia\"),\n",
    "        ('washington', 'united states', \"north america\"),\n",
    "        ('chicago', 'united states', \"north america\"),\n",
    "        ('san francisco', 'united states', \"north america\"),\n",
    "        ('toronto', 'canada', \"north america\"),\n",
    "        ('seattle', 'united states', \"north america\"),\n",
    "        ('atlanta', 'united states', \"north america\"),\n",
    "        ('Nashville', 'united states', \"north america\"),\n",
    "    ]\n",
    "\n",
    "    location = location.replace('ny', 'new york')\n",
    "    location = location.replace('nyc', 'new york')\n",
    "    location = location.replace('new york city', 'new york')\n",
    "    location = location.replace('la', 'los angeles')\n",
    "\n",
    "    location = location.replace('uk', 'united kingdom')\n",
    "    location = location.replace('usa', 'united states')\n",
    "\n",
    "    \n",
    "    res = \"unknown\"\n",
    "    for city, country, continent in cities:\n",
    "        if city in location:\n",
    "            return pd.Series([city, country, continent])\n",
    "    for country, continent in countries:\n",
    "        if country in location:\n",
    "            return pd.Series([\"unknown\", country, continent])\n",
    "    for continent in continents:\n",
    "        if continent in location:\n",
    "            return pd.Series([\"unknown\", \"unknown\", continent])\n",
    "        \n",
    "    return pd.Series([\"unknown\", \"unknown\", \"unknown\"])\n",
    "\n",
    "def location_preprocessing(df):\n",
    "    df[\"location_cleaned\"] = df[\"location\"].copy()\n",
    "    # Lower\n",
    "    df[\"location_cleaned\"] = df[\"location_cleaned\"].str.lower()\n",
    "    # Remove trailing sapces\n",
    "    df['location_cleaned'] = df['location_cleaned'].str.strip()\n",
    "    # Fill NaN values\n",
    "    df[\"location_cleaned\"] = df[\"location_cleaned\"].fillna(\"unknown\")\n",
    "    # Replace everywhere / worlwide by unknown\n",
    "    df[\"location_cleaned\"] = df[\"location_cleaned\"].str.replace(\"everywhere\", \"unknown\")\n",
    "    df[\"location_cleaned\"] = df[\"location_cleaned\"].str.replace(\"worlwide\", \"unknown\")\n",
    "    df[\"location_cleaned\"] = df[\"location_cleaned\"].str.replace(\"earth\", \"unknown\")\n",
    "    # Extract continents\n",
    "    df[[\"city\", \"country\", \"continent\"]] = df[\"location_cleaned\"].apply(extract_cities_countries_continents)\n",
    "    # Post\n",
    "    \n",
    "    return df\n",
    "\n",
    "def keyword_preprocessing(df):\n",
    "    df[\"keyword_cleaned\"] = df[\"keyword\"].copy()\n",
    "    # Lower\n",
    "    df[\"keyword_cleaned\"] = df[\"keyword_cleaned\"].str.lower()\n",
    "    # Remove trailing sapces\n",
    "    df['keyword_cleaned'] = df['keyword_cleaned'].str.strip()\n",
    "    # Fill NaN values\n",
    "    df[\"keyword_cleaned\"] = df[\"keyword_cleaned\"].fillna(\"unknown\")\n",
    "    # Replace %20 by spaces\n",
    "    df[\"keyword_cleaned\"] = df[\"keyword_cleaned\"].str.replace(\"%20\", \" \")\n",
    "    # Create a column to indicate if the keyword is relevant\n",
    "    df[\"keyword_in_text\"] = df.apply(lambda row: row[\"keyword_cleaned\"] in row[\"text_cleaned\"],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = text_preprocessing(train)\n",
    "test = text_preprocessing(test)\n",
    "\n",
    "train = location_preprocessing(train)\n",
    "test = location_preprocessing(test)\n",
    "\n",
    "train = keyword_preprocessing(train)\n",
    "test = keyword_preprocessing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword_in_text\n",
       "True     6634\n",
       "False     979\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"keyword_in_text\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>location_cleaned</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>continent</th>\n",
       "      <th>keyword_cleaned</th>\n",
       "      <th>keyword_in_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>wholesale markets ablaze</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>we always try to bring the heavy metal rt</td>\n",
       "      <td>est. september 2012 - bristol</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>africanbaze breaking newsnigeria flag set abla...</td>\n",
       "      <td>africa</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>africa</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>crying out for more set me ablaze</td>\n",
       "      <td>philadelphia, pa</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>united states</td>\n",
       "      <td>north america</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>on plus side look at the sky last night it was...</td>\n",
       "      <td>london, uk</td>\n",
       "      <td>london</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>europe</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "      <td>and i wrecked you both</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>three days off from work and theyve pretty muc...</td>\n",
       "      <td>vancouver, canada</td>\n",
       "      <td>unknown</td>\n",
       "      <td>canada</td>\n",
       "      <td>north america</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>fx forex trading cramer igers 3 words that wre...</td>\n",
       "      <td>london</td>\n",
       "      <td>london</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>europe</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "      <td>great atmosphere at the british lion gig tonig...</td>\n",
       "      <td>lincoln</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words that wrecked disneys stoc...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7578  10830  wrecked                            NaN   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "7582  10834  wrecked                            NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
       "32    We always try to bring the heavy. #metal #RT h...       0   \n",
       "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n",
       "34                   Crying out for more! Set me ablaze       0   \n",
       "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n",
       "...                                                 ...     ...   \n",
       "7578   @jt_ruff23 @cameronhacker and I wrecked you both       0   \n",
       "7579  Three days off from work and they've pretty mu...       0   \n",
       "7580  #FX #forex #trading Cramer: Iger's 3 words tha...       0   \n",
       "7581  @engineshed Great atmosphere at the British Li...       0   \n",
       "7582  Cramer: Iger's 3 words that wrecked Disney's s...       0   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "31                             wholesale markets ablaze   \n",
       "32            we always try to bring the heavy metal rt   \n",
       "33    africanbaze breaking newsnigeria flag set abla...   \n",
       "34                    crying out for more set me ablaze   \n",
       "35    on plus side look at the sky last night it was...   \n",
       "...                                                 ...   \n",
       "7578                             and i wrecked you both   \n",
       "7579  three days off from work and theyve pretty muc...   \n",
       "7580  fx forex trading cramer igers 3 words that wre...   \n",
       "7581  great atmosphere at the british lion gig tonig...   \n",
       "7582  cramer igers 3 words that wrecked disneys stoc...   \n",
       "\n",
       "                   location_cleaned         city         country  \\\n",
       "31                       birmingham      unknown         unknown   \n",
       "32    est. september 2012 - bristol      unknown         unknown   \n",
       "33                           africa      unknown         unknown   \n",
       "34                 philadelphia, pa  los angeles   united states   \n",
       "35                       london, uk       london  united kingdom   \n",
       "...                             ...          ...             ...   \n",
       "7578                        unknown      unknown         unknown   \n",
       "7579              vancouver, canada      unknown          canada   \n",
       "7580                         london       london  united kingdom   \n",
       "7581                        lincoln      unknown         unknown   \n",
       "7582                        unknown      unknown         unknown   \n",
       "\n",
       "          continent keyword_cleaned  keyword_in_text  \n",
       "31          unknown          ablaze             True  \n",
       "32          unknown          ablaze            False  \n",
       "33           africa          ablaze             True  \n",
       "34    north america          ablaze             True  \n",
       "35           europe          ablaze             True  \n",
       "...             ...             ...              ...  \n",
       "7578        unknown         wrecked             True  \n",
       "7579  north america         wrecked             True  \n",
       "7580         europe         wrecked             True  \n",
       "7581        unknown         wrecked             True  \n",
       "7582        unknown         wrecked             True  \n",
       "\n",
       "[7552 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dropna(subset=[\"keyword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vetorizer = TfidfVectorizer()\n",
    "train_vectors = tfidf_vetorizer.fit_transform(train[\"text_cleaned\"])\n",
    "test_vectors = tfidf_vetorizer.transform(test[\"text_cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], shape=(7613, 15741))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn. model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Logistic Regression F1 scores: [0.51821862 0.43544304 0.54586808 0.53846154 0.66904932]\n",
      "Logistic Regression Mean F1: 0.5414\n",
      "\n",
      "=== Linear SVM ===\n",
      "Linear SVM F1 scores: [0.56428571 0.44115355 0.57894737 0.56872038 0.67104399]\n",
      "Linear SVM Mean F1: 0.5648\n",
      "\n",
      "=== Ridge Classifier ===\n",
      "Ridge Classifier F1 scores: [0.56505576 0.44217152 0.56735567 0.57027464 0.67204301]\n",
      "Ridge Classifier Mean F1: 0.5634\n",
      "\n",
      "=== Passive Aggressive ===\n",
      "Passive Aggressive F1 scores: [0.53776978 0.4169279  0.56       0.56498873 0.64190981]\n",
      "Passive Aggressive Mean F1: 0.5443\n",
      "\n",
      "=== SGD (Log Loss) ===\n",
      "SGD (Log Loss) F1 scores: [0.57070279 0.42517007 0.55322339 0.54637437 0.68082368]\n",
      "SGD (Log Loss) Mean F1: 0.5553\n"
     ]
    }
   ],
   "source": [
    "# Define your models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Linear SVM\": LinearSVC(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"Passive Aggressive\": PassiveAggressiveClassifier(),\n",
    "    \"SGD (Log Loss)\": SGDClassifier(loss='log_loss'),\n",
    "}\n",
    "\n",
    "# Columns\n",
    "text_col = 'text_cleaned'\n",
    "city_col = 'city'\n",
    "country_col = 'country'\n",
    "continent_col = 'continent'\n",
    "keyword_col = 'keyword'\n",
    "keyword_in_text_col = 'keyword_in_text'\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    # Preprocessing pipeline for current model\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), text_col),\n",
    "        ('city_ohe', OneHotEncoder(handle_unknown='ignore'), [city_col]),\n",
    "        ('country_ohe', OneHotEncoder(handle_unknown='ignore'), [country_col]),\n",
    "        ('continent_ohe', OneHotEncoder(handle_unknown='ignore'), [continent_col]),\n",
    "        ('keyword_ohe', OneHotEncoder(handle_unknown='ignore'), [keyword_col]),\n",
    "        ('keyword_in_text_ohe', OneHotEncoder(handle_unknown='ignore'), [keyword_in_text_col]),\n",
    "\n",
    "    ])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(pipeline, train, train[\"target\"], cv=5, scoring=\"f1\")\n",
    "    print(f\"{name} F1 scores: {scores}\")\n",
    "    print(f\"{name} Mean F1: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYZE ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[[\"keyword\", \"location\", \"text_cleaned\"]], train[\"target\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Columns\n",
    "text_col = 'text_cleaned'\n",
    "keyword_col = 'keyword'\n",
    "location_col = 'location'\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('tfidf', TfidfVectorizer(), text_col),\n",
    "        ('keyword_ohe', OneHotEncoder(handle_unknown='ignore'), [keyword_col]),\n",
    "        ('location_ohe', OneHotEncoder(handle_unknown='ignore'), [location_col]),\n",
    "    ],\n",
    "    remainder='drop'  # drop other columns\n",
    ")\n",
    "\n",
    "# Full pipeline with classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit on training data (pandas DataFrame)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"keyword\": X_test[\"keyword\"],\n",
    "    \"location\": X_test[\"location\"],\n",
    "    \"text\": X_test[\"text_cleaned\"],\n",
    "    \"true_label\": y_test,\n",
    "    \"predicted\": y_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " false positive: (139, 5)\n",
      " true positive: (463, 5)\n",
      " false negative: (186, 5)\n",
      " true negative: (735, 5)\n"
     ]
    }
   ],
   "source": [
    "false_positive = results[(results[\"true_label\"] == 0) & (results[\"predicted\"] == 1)]\n",
    "true_positive = results[(results[\"true_label\"] == 1) & (results[\"predicted\"] == 1)]\n",
    "false_negative = results[(results[\"true_label\"] == 1) & (results[\"predicted\"] == 0)]\n",
    "true_negative = results[(results[\"true_label\"] == 0) & (results[\"predicted\"] == 0)]\n",
    "\n",
    "print(f\" false positive: {false_positive.shape}\")\n",
    "print(f\" true positive: {true_positive.shape}\")\n",
    "print(f\" false negative: {false_negative.shape}\")\n",
    "print(f\" true negative: {true_negative.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword\n",
       "hazardous        5\n",
       "crashed          4\n",
       "flooding         4\n",
       "trauma           4\n",
       "floods           3\n",
       "                ..\n",
       "evacuation       1\n",
       "emergency        1\n",
       "bombed           1\n",
       "mass%20murder    1\n",
       "crash            1\n",
       "Name: count, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive[\"keyword\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>dt rt the col police can catch a pickpocket in...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>hijacking</td>\n",
       "      <td>Athens,Greece</td>\n",
       "      <td>the murderous story of americas first hijacking</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>crash</td>\n",
       "      <td>In my own world!!!</td>\n",
       "      <td>akilah world news cop pulls man from car to av...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>debris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>malaysia airlines flight 370 that disappeared ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>nepal earthquake 3 months on women fear abuse via</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword              location  \\\n",
       "5448      police                    UK   \n",
       "4398   hijacking         Athens,Greece   \n",
       "1807       crash    In my own world!!!   \n",
       "2164      debris                   NaN   \n",
       "3044  earthquake  Melbourne, Australia   \n",
       "\n",
       "                                                   text  true_label  predicted  \n",
       "5448  dt rt the col police can catch a pickpocket in...           1          1  \n",
       "4398    the murderous story of americas first hijacking           1          1  \n",
       "1807  akilah world news cop pulls man from car to av...           1          1  \n",
       "2164  malaysia airlines flight 370 that disappeared ...           1          1  \n",
       "3044  nepal earthquake 3 months on women fear abuse via           1          1  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_positive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 3)\n",
      "(350, 3)\n"
     ]
    }
   ],
   "source": [
    "# Texts with links\n",
    "print(false_positive[false_positive[\"text\"].str.contains(\"http://\")].shape)\n",
    "print(true_positive[true_positive[\"text\"].str.contains(\"http://\")].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission_template.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
